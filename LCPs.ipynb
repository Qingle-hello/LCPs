{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from math import pi, cos\n",
    "from scipy.special import factorial\n",
    "from sympy import symbols, lambdify\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "from sympy import simplify\n",
    "from scipy.sparse import diags\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")  # Check for CUDA availability\n",
    "\n",
    "m = 2  # degree of polynomial Q(x)\n",
    "n = 2  # degree of polynomial P(x)\n",
    "q = 1  # degree of truncated Taylor series,q<=n\n",
    "J = 16  # for loss_1\n",
    "N = 2000  # for lambda values in loss_1\n",
    "h = 1/N  # for lambda values in loss_1\n",
    "p2 = 1  # penalty term for loss_2\n",
    "epochs = 50\n",
    "K = 500 # loops in one epoch\n",
    "\n",
    "# coefficients of exp(-x) Taylor expansion\n",
    "c = torch.tensor([(-1)**i / factorial(i) for i in range(q+1)], dtype=torch.float32, device=device)\n",
    "\n",
    "# PadÃ© approximation parameters to be learned\n",
    "a2 = nn.Parameter(torch.zeros(n-q, device=device), requires_grad=True)\n",
    "b1_log = nn.Parameter(torch.zeros(q, device=device), requires_grad=True)\n",
    "b2_log = nn.Parameter(torch.ones(m-q, device=device), requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(x):\n",
    "    \"\"\"\n",
    "    Polynomial function with coefficients a = [c*B, a2]\n",
    "    \"\"\"\n",
    "    B = torch.cat((torch.ones(1, device=device), torch.exp(b1_log)))\n",
    "    c_B = torch.zeros_like(B, device=device)\n",
    "    for k in range(len(B)):\n",
    "        for j in range(k+1):\n",
    "            c_B[k] += c[j] * B[k-j]\n",
    "    a = torch.cat((c_B, a2))\n",
    "    p = torch.zeros_like(x, device=device)\n",
    "    for coeff in a.flip(0):  # Reverse the tensor for Horner's method\n",
    "        p = p * x + coeff\n",
    "    return p\n",
    "\n",
    "def Q(x):\n",
    "    \"\"\"\n",
    "    Polynomial function with coefficients b = [B, b2]\n",
    "    \"\"\"\n",
    "    B = torch.cat((torch.ones(1, device=device), torch.exp(b1_log)))\n",
    "    b = torch.cat((B, torch.exp(b2_log)))\n",
    "    q = torch.zeros_like(x, device=device)\n",
    "    for coeff in b.flip(0):  # Reverse the tensor for Horner's method\n",
    "        q = q * x + coeff\n",
    "    return q\n",
    "\n",
    "# LCPs\n",
    "def R(x):\n",
    "    return P(x) / Q(x)\n",
    "\n",
    "# two-stage Lobatto IIIC \n",
    "def r2(x):\n",
    "    return (2) / (x ** 2 + 2 * x + 2) \n",
    "\n",
    "# three-stage Lobatto IIIC\n",
    "def r3(x):\n",
    "    return (-6 * x + 24) / (x ** 3 + 6 * x ** 2 + 18 * x + 24)\n",
    "\n",
    "# four-stage Lobatto IIIC\n",
    "def r4(x):\n",
    "    return (12*x**2 - 120*x + 360) / (x**4 + 12*x**3 + 72*x**2 + 240*x + 360)\n",
    "\n",
    "# three-stage Radau IIA\n",
    "def rc(s):\n",
    "    b = 0.5 * (1 + np.sqrt(3) / 3)\n",
    "    return 1 - s / (1 + b * s) - np.sqrt(3) / 6 * (s / (1 + b * s))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, epoch:0, Loss1: 0.275765, Loss2: -0.005858, Total Loss: 0.281037\n",
      "Iteration 0, epoch:100, Loss1: 0.062654, Loss2: -0.010006, Total Loss: 0.071659\n",
      "Iteration 0, epoch:200, Loss1: 0.059199, Loss2: -0.009941, Total Loss: 0.068146\n",
      "Iteration 0, epoch:300, Loss1: 0.056779, Loss2: -0.009772, Total Loss: 0.065574\n",
      "Iteration 0, epoch:400, Loss1: 0.055450, Loss2: -0.009795, Total Loss: 0.064265\n",
      "Iteration 1, epoch:0, Loss1: 0.055222, Loss2: -0.009783, Total Loss: 0.063146\n",
      "Iteration 1, epoch:100, Loss1: 0.055245, Loss2: -0.009753, Total Loss: 0.063145\n",
      "Iteration 1, epoch:200, Loss1: 0.055379, Loss2: -0.009665, Total Loss: 0.063208\n",
      "Iteration 1, epoch:300, Loss1: 0.052465, Loss2: -0.009598, Total Loss: 0.060239\n",
      "Iteration 1, epoch:400, Loss1: 0.050170, Loss2: -0.009434, Total Loss: 0.057811\n",
      "Iteration 2, epoch:0, Loss1: 0.044287, Loss2: -0.009314, Total Loss: 0.051076\n",
      "Iteration 2, epoch:100, Loss1: 0.038906, Loss2: -0.008629, Total Loss: 0.045197\n",
      "Iteration 2, epoch:200, Loss1: 0.031615, Loss2: -0.008796, Total Loss: 0.038027\n",
      "Iteration 2, epoch:300, Loss1: 0.027378, Loss2: -0.008709, Total Loss: 0.033727\n",
      "Iteration 2, epoch:400, Loss1: 0.025747, Loss2: -0.008659, Total Loss: 0.032060\n",
      "Iteration 3, epoch:0, Loss1: 0.025313, Loss2: -0.008643, Total Loss: 0.030983\n",
      "Iteration 3, epoch:100, Loss1: 0.025379, Loss2: -0.008618, Total Loss: 0.031033\n",
      "Iteration 3, epoch:200, Loss1: 0.024153, Loss2: -0.008580, Total Loss: 0.029782\n",
      "Iteration 3, epoch:300, Loss1: 0.021882, Loss2: -0.008351, Total Loss: 0.027361\n",
      "Iteration 3, epoch:400, Loss1: 0.018235, Loss2: -0.008199, Total Loss: 0.023615\n",
      "Iteration 4, epoch:0, Loss1: 0.015900, Loss2: -0.008172, Total Loss: 0.020726\n",
      "Iteration 4, epoch:100, Loss1: 0.016330, Loss2: -0.008162, Total Loss: 0.021149\n",
      "Iteration 4, epoch:200, Loss1: 0.015418, Loss2: -0.008145, Total Loss: 0.020227\n",
      "Iteration 4, epoch:300, Loss1: 0.013851, Loss2: -0.008169, Total Loss: 0.018675\n",
      "Iteration 4, epoch:400, Loss1: 0.013614, Loss2: -0.008168, Total Loss: 0.018437\n",
      "Iteration 5, epoch:0, Loss1: 0.013402, Loss2: -0.008168, Total Loss: 0.017743\n",
      "Iteration 5, epoch:100, Loss1: 0.014035, Loss2: -0.008163, Total Loss: 0.018373\n",
      "Iteration 5, epoch:200, Loss1: 0.013801, Loss2: -0.008170, Total Loss: 0.018143\n",
      "Iteration 5, epoch:300, Loss1: 0.015201, Loss2: -0.008143, Total Loss: 0.019529\n",
      "Iteration 5, epoch:400, Loss1: 0.013907, Loss2: -0.008165, Total Loss: 0.018246\n",
      "Iteration 6, epoch:0, Loss1: 0.014333, Loss2: -0.008156, Total Loss: 0.018234\n",
      "Iteration 6, epoch:100, Loss1: 0.015693, Loss2: -0.008182, Total Loss: 0.019606\n",
      "Iteration 6, epoch:200, Loss1: 0.014560, Loss2: -0.008182, Total Loss: 0.018474\n",
      "Iteration 6, epoch:300, Loss1: 0.014966, Loss2: -0.008163, Total Loss: 0.018871\n",
      "Iteration 6, epoch:400, Loss1: 0.013412, Loss2: -0.008169, Total Loss: 0.017319\n",
      "Iteration 7, epoch:0, Loss1: 0.013402, Loss2: -0.008168, Total Loss: 0.016918\n",
      "Iteration 7, epoch:100, Loss1: 0.013618, Loss2: -0.008165, Total Loss: 0.017133\n",
      "Iteration 7, epoch:200, Loss1: 0.014446, Loss2: -0.008176, Total Loss: 0.017965\n",
      "Iteration 7, epoch:300, Loss1: 0.016797, Loss2: -0.008214, Total Loss: 0.020333\n",
      "Iteration 7, epoch:400, Loss1: 0.015900, Loss2: -0.008190, Total Loss: 0.019425\n",
      "Iteration 8, epoch:0, Loss1: 0.014295, Loss2: -0.008174, Total Loss: 0.017462\n",
      "Iteration 8, epoch:100, Loss1: 0.016860, Loss2: -0.008145, Total Loss: 0.020016\n",
      "Iteration 8, epoch:200, Loss1: 0.014018, Loss2: -0.008163, Total Loss: 0.017181\n",
      "Iteration 8, epoch:300, Loss1: 0.013922, Loss2: -0.008170, Total Loss: 0.017088\n",
      "Iteration 8, epoch:400, Loss1: 0.013648, Loss2: -0.008169, Total Loss: 0.016813\n",
      "Iteration 9, epoch:0, Loss1: 0.013386, Loss2: -0.008168, Total Loss: 0.016234\n",
      "Iteration 9, epoch:100, Loss1: 0.013506, Loss2: -0.008169, Total Loss: 0.016354\n",
      "Iteration 9, epoch:200, Loss1: 0.014194, Loss2: -0.008157, Total Loss: 0.017038\n",
      "Iteration 9, epoch:300, Loss1: 0.014077, Loss2: -0.008174, Total Loss: 0.016927\n",
      "Iteration 9, epoch:400, Loss1: 0.015674, Loss2: -0.008194, Total Loss: 0.018531\n",
      "Iteration 10, epoch:0, Loss1: 0.017462, Loss2: -0.008189, Total Loss: 0.020032\n",
      "Iteration 10, epoch:100, Loss1: 0.017840, Loss2: -0.008212, Total Loss: 0.020417\n",
      "Iteration 10, epoch:200, Loss1: 0.014646, Loss2: -0.008152, Total Loss: 0.017205\n",
      "Iteration 10, epoch:300, Loss1: 0.014253, Loss2: -0.008153, Total Loss: 0.016812\n",
      "Iteration 10, epoch:400, Loss1: 0.013629, Loss2: -0.008169, Total Loss: 0.016192\n",
      "Iteration 11, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.015697\n",
      "Iteration 11, epoch:100, Loss1: 0.013661, Loss2: -0.008164, Total Loss: 0.015967\n",
      "Iteration 11, epoch:200, Loss1: 0.014509, Loss2: -0.008155, Total Loss: 0.016812\n",
      "Iteration 11, epoch:300, Loss1: 0.016278, Loss2: -0.008146, Total Loss: 0.018579\n",
      "Iteration 11, epoch:400, Loss1: 0.016285, Loss2: -0.008158, Total Loss: 0.018589\n",
      "Iteration 12, epoch:0, Loss1: 0.016431, Loss2: -0.008193, Total Loss: 0.018513\n",
      "Iteration 12, epoch:100, Loss1: 0.014429, Loss2: -0.008178, Total Loss: 0.016508\n",
      "Iteration 12, epoch:200, Loss1: 0.014305, Loss2: -0.008158, Total Loss: 0.016379\n",
      "Iteration 12, epoch:300, Loss1: 0.013936, Loss2: -0.008170, Total Loss: 0.016012\n",
      "Iteration 12, epoch:400, Loss1: 0.013510, Loss2: -0.008169, Total Loss: 0.015586\n",
      "Iteration 13, epoch:0, Loss1: 0.013391, Loss2: -0.008168, Total Loss: 0.015260\n",
      "Iteration 13, epoch:100, Loss1: 0.013455, Loss2: -0.008169, Total Loss: 0.015324\n",
      "Iteration 13, epoch:200, Loss1: 0.014604, Loss2: -0.008162, Total Loss: 0.016471\n",
      "Iteration 13, epoch:300, Loss1: 0.015332, Loss2: -0.008185, Total Loss: 0.017204\n",
      "Iteration 13, epoch:400, Loss1: 0.014525, Loss2: -0.008168, Total Loss: 0.016394\n",
      "Iteration 14, epoch:0, Loss1: 0.016953, Loss2: -0.008141, Total Loss: 0.018629\n",
      "Iteration 14, epoch:100, Loss1: 0.014777, Loss2: -0.008159, Total Loss: 0.016457\n",
      "Iteration 14, epoch:200, Loss1: 0.014017, Loss2: -0.008172, Total Loss: 0.015700\n",
      "Iteration 14, epoch:300, Loss1: 0.014063, Loss2: -0.008171, Total Loss: 0.015745\n",
      "Iteration 14, epoch:400, Loss1: 0.013889, Loss2: -0.008173, Total Loss: 0.015572\n",
      "Iteration 15, epoch:0, Loss1: 0.013392, Loss2: -0.008168, Total Loss: 0.014905\n",
      "Iteration 15, epoch:100, Loss1: 0.013611, Loss2: -0.008170, Total Loss: 0.015125\n",
      "Iteration 15, epoch:200, Loss1: 0.014873, Loss2: -0.008159, Total Loss: 0.016385\n",
      "Iteration 15, epoch:300, Loss1: 0.015852, Loss2: -0.008159, Total Loss: 0.017364\n",
      "Iteration 15, epoch:400, Loss1: 0.014214, Loss2: -0.008167, Total Loss: 0.015728\n",
      "Iteration 16, epoch:0, Loss1: 0.015793, Loss2: -0.008186, Total Loss: 0.017159\n",
      "Iteration 16, epoch:100, Loss1: 0.015874, Loss2: -0.008136, Total Loss: 0.017231\n",
      "Iteration 16, epoch:200, Loss1: 0.015349, Loss2: -0.008179, Total Loss: 0.016713\n",
      "Iteration 16, epoch:300, Loss1: 0.014370, Loss2: -0.008174, Total Loss: 0.015733\n",
      "Iteration 16, epoch:400, Loss1: 0.013976, Loss2: -0.008161, Total Loss: 0.015337\n",
      "Iteration 17, epoch:0, Loss1: 0.013403, Loss2: -0.008168, Total Loss: 0.014629\n",
      "Iteration 17, epoch:100, Loss1: 0.013942, Loss2: -0.008171, Total Loss: 0.015169\n",
      "Iteration 17, epoch:200, Loss1: 0.014348, Loss2: -0.008156, Total Loss: 0.015572\n",
      "Iteration 17, epoch:300, Loss1: 0.014978, Loss2: -0.008173, Total Loss: 0.016205\n",
      "Iteration 17, epoch:400, Loss1: 0.015471, Loss2: -0.008153, Total Loss: 0.016694\n",
      "Iteration 18, epoch:0, Loss1: 0.016108, Loss2: -0.008152, Total Loss: 0.017210\n",
      "Iteration 18, epoch:100, Loss1: 0.016148, Loss2: -0.008195, Total Loss: 0.017255\n",
      "Iteration 18, epoch:200, Loss1: 0.014324, Loss2: -0.008154, Total Loss: 0.015425\n",
      "Iteration 18, epoch:300, Loss1: 0.014867, Loss2: -0.008162, Total Loss: 0.015970\n",
      "Iteration 18, epoch:400, Loss1: 0.013583, Loss2: -0.008167, Total Loss: 0.014686\n",
      "Iteration 19, epoch:0, Loss1: 0.013388, Loss2: -0.008168, Total Loss: 0.014381\n",
      "Iteration 19, epoch:100, Loss1: 0.013790, Loss2: -0.008162, Total Loss: 0.014783\n",
      "Iteration 19, epoch:200, Loss1: 0.014155, Loss2: -0.008155, Total Loss: 0.015146\n",
      "Iteration 19, epoch:300, Loss1: 0.013796, Loss2: -0.008163, Total Loss: 0.014788\n",
      "Iteration 19, epoch:400, Loss1: 0.015281, Loss2: -0.008148, Total Loss: 0.016272\n",
      "Iteration 20, epoch:0, Loss1: 0.015391, Loss2: -0.008143, Total Loss: 0.016282\n",
      "Iteration 20, epoch:100, Loss1: 0.015979, Loss2: -0.008145, Total Loss: 0.016871\n",
      "Iteration 20, epoch:200, Loss1: 0.015305, Loss2: -0.008164, Total Loss: 0.016199\n",
      "Iteration 20, epoch:300, Loss1: 0.013920, Loss2: -0.008163, Total Loss: 0.014813\n",
      "Iteration 20, epoch:400, Loss1: 0.013626, Loss2: -0.008166, Total Loss: 0.014520\n",
      "Iteration 21, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.014194\n",
      "Iteration 21, epoch:100, Loss1: 0.013433, Loss2: -0.008168, Total Loss: 0.014237\n",
      "Iteration 21, epoch:200, Loss1: 0.014235, Loss2: -0.008164, Total Loss: 0.015039\n",
      "Iteration 21, epoch:300, Loss1: 0.014730, Loss2: -0.008176, Total Loss: 0.015536\n",
      "Iteration 21, epoch:400, Loss1: 0.014364, Loss2: -0.008152, Total Loss: 0.015167\n",
      "Iteration 22, epoch:0, Loss1: 0.014350, Loss2: -0.008158, Total Loss: 0.015073\n",
      "Iteration 22, epoch:100, Loss1: 0.014431, Loss2: -0.008154, Total Loss: 0.015154\n",
      "Iteration 22, epoch:200, Loss1: 0.014547, Loss2: -0.008161, Total Loss: 0.015270\n",
      "Iteration 22, epoch:300, Loss1: 0.015180, Loss2: -0.008157, Total Loss: 0.015903\n",
      "Iteration 22, epoch:400, Loss1: 0.013658, Loss2: -0.008170, Total Loss: 0.014382\n",
      "Iteration 23, epoch:0, Loss1: 0.013397, Loss2: -0.008168, Total Loss: 0.014048\n",
      "Iteration 23, epoch:100, Loss1: 0.013565, Loss2: -0.008169, Total Loss: 0.014217\n",
      "Iteration 23, epoch:200, Loss1: 0.013907, Loss2: -0.008170, Total Loss: 0.014559\n",
      "Iteration 23, epoch:300, Loss1: 0.015689, Loss2: -0.008154, Total Loss: 0.016339\n",
      "Iteration 23, epoch:400, Loss1: 0.015729, Loss2: -0.008185, Total Loss: 0.016382\n",
      "Iteration 24, epoch:0, Loss1: 0.014944, Loss2: -0.008146, Total Loss: 0.015529\n",
      "Iteration 24, epoch:100, Loss1: 0.014450, Loss2: -0.008170, Total Loss: 0.015037\n",
      "Iteration 24, epoch:200, Loss1: 0.015717, Loss2: -0.008192, Total Loss: 0.016305\n",
      "Iteration 24, epoch:300, Loss1: 0.014457, Loss2: -0.008168, Total Loss: 0.015044\n",
      "Iteration 24, epoch:400, Loss1: 0.013747, Loss2: -0.008165, Total Loss: 0.014333\n",
      "Iteration 25, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.013918\n",
      "Iteration 25, epoch:100, Loss1: 0.013581, Loss2: -0.008165, Total Loss: 0.014108\n",
      "Iteration 25, epoch:200, Loss1: 0.013907, Loss2: -0.008172, Total Loss: 0.014435\n",
      "Iteration 25, epoch:300, Loss1: 0.014803, Loss2: -0.008157, Total Loss: 0.015330\n",
      "Iteration 25, epoch:400, Loss1: 0.016086, Loss2: -0.008191, Total Loss: 0.016616\n",
      "Iteration 26, epoch:0, Loss1: 0.015873, Loss2: -0.008137, Total Loss: 0.016346\n",
      "Iteration 26, epoch:100, Loss1: 0.014548, Loss2: -0.008172, Total Loss: 0.015023\n",
      "Iteration 26, epoch:200, Loss1: 0.015108, Loss2: -0.008141, Total Loss: 0.015581\n",
      "Iteration 26, epoch:300, Loss1: 0.014681, Loss2: -0.008174, Total Loss: 0.015156\n",
      "Iteration 26, epoch:400, Loss1: 0.013548, Loss2: -0.008166, Total Loss: 0.014023\n",
      "Iteration 27, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.013817\n",
      "Iteration 27, epoch:100, Loss1: 0.013822, Loss2: -0.008170, Total Loss: 0.014250\n",
      "Iteration 27, epoch:200, Loss1: 0.014441, Loss2: -0.008178, Total Loss: 0.014868\n",
      "Iteration 27, epoch:300, Loss1: 0.014970, Loss2: -0.008188, Total Loss: 0.015399\n",
      "Iteration 27, epoch:400, Loss1: 0.015005, Loss2: -0.008180, Total Loss: 0.015433\n",
      "Iteration 28, epoch:0, Loss1: 0.016803, Loss2: -0.008138, Total Loss: 0.017187\n",
      "Iteration 28, epoch:100, Loss1: 0.015337, Loss2: -0.008147, Total Loss: 0.015721\n",
      "Iteration 28, epoch:200, Loss1: 0.015025, Loss2: -0.008143, Total Loss: 0.015408\n",
      "Iteration 28, epoch:300, Loss1: 0.013896, Loss2: -0.008170, Total Loss: 0.014281\n",
      "Iteration 28, epoch:400, Loss1: 0.013700, Loss2: -0.008170, Total Loss: 0.014085\n",
      "Iteration 29, epoch:0, Loss1: 0.013395, Loss2: -0.008168, Total Loss: 0.013741\n",
      "Iteration 29, epoch:100, Loss1: 0.013619, Loss2: -0.008169, Total Loss: 0.013965\n",
      "Iteration 29, epoch:200, Loss1: 0.014117, Loss2: -0.008172, Total Loss: 0.014464\n",
      "Iteration 29, epoch:300, Loss1: 0.015369, Loss2: -0.008183, Total Loss: 0.015716\n",
      "Iteration 29, epoch:400, Loss1: 0.016784, Loss2: -0.008196, Total Loss: 0.017132\n",
      "Iteration 30, epoch:0, Loss1: 0.019193, Loss2: -0.008175, Total Loss: 0.019505\n",
      "Iteration 30, epoch:100, Loss1: 0.014801, Loss2: -0.008161, Total Loss: 0.015112\n",
      "Iteration 30, epoch:200, Loss1: 0.014606, Loss2: -0.008156, Total Loss: 0.014918\n",
      "Iteration 30, epoch:300, Loss1: 0.014044, Loss2: -0.008164, Total Loss: 0.014356\n",
      "Iteration 30, epoch:400, Loss1: 0.013788, Loss2: -0.008169, Total Loss: 0.014100\n",
      "Iteration 31, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.013670\n",
      "Iteration 31, epoch:100, Loss1: 0.013547, Loss2: -0.008166, Total Loss: 0.013827\n",
      "Iteration 31, epoch:200, Loss1: 0.013773, Loss2: -0.008163, Total Loss: 0.014053\n",
      "Iteration 31, epoch:300, Loss1: 0.014966, Loss2: -0.008181, Total Loss: 0.015247\n",
      "Iteration 31, epoch:400, Loss1: 0.015970, Loss2: -0.008147, Total Loss: 0.016250\n",
      "Iteration 32, epoch:0, Loss1: 0.015208, Loss2: -0.008182, Total Loss: 0.015461\n",
      "Iteration 32, epoch:100, Loss1: 0.018610, Loss2: -0.008239, Total Loss: 0.018864\n",
      "Iteration 32, epoch:200, Loss1: 0.014595, Loss2: -0.008180, Total Loss: 0.014848\n",
      "Iteration 32, epoch:300, Loss1: 0.013784, Loss2: -0.008162, Total Loss: 0.014036\n",
      "Iteration 32, epoch:400, Loss1: 0.013947, Loss2: -0.008162, Total Loss: 0.014199\n",
      "Iteration 33, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.013617\n",
      "Iteration 33, epoch:100, Loss1: 0.013591, Loss2: -0.008166, Total Loss: 0.013818\n",
      "Iteration 33, epoch:200, Loss1: 0.014410, Loss2: -0.008161, Total Loss: 0.014637\n",
      "Iteration 33, epoch:300, Loss1: 0.013831, Loss2: -0.008167, Total Loss: 0.014058\n",
      "Iteration 33, epoch:400, Loss1: 0.016498, Loss2: -0.008186, Total Loss: 0.016726\n",
      "Iteration 34, epoch:0, Loss1: 0.017692, Loss2: -0.008144, Total Loss: 0.017896\n",
      "Iteration 34, epoch:100, Loss1: 0.014523, Loss2: -0.008161, Total Loss: 0.014728\n",
      "Iteration 34, epoch:200, Loss1: 0.015842, Loss2: -0.008147, Total Loss: 0.016046\n",
      "Iteration 34, epoch:300, Loss1: 0.014293, Loss2: -0.008162, Total Loss: 0.014497\n",
      "Iteration 34, epoch:400, Loss1: 0.013493, Loss2: -0.008167, Total Loss: 0.013698\n",
      "Iteration 35, epoch:0, Loss1: 0.013397, Loss2: -0.008168, Total Loss: 0.013581\n",
      "Iteration 35, epoch:100, Loss1: 0.013521, Loss2: -0.008168, Total Loss: 0.013705\n",
      "Iteration 35, epoch:200, Loss1: 0.013685, Loss2: -0.008163, Total Loss: 0.013869\n",
      "Iteration 35, epoch:300, Loss1: 0.014604, Loss2: -0.008158, Total Loss: 0.014788\n",
      "Iteration 35, epoch:400, Loss1: 0.015241, Loss2: -0.008178, Total Loss: 0.015425\n",
      "Iteration 36, epoch:0, Loss1: 0.014931, Loss2: -0.008173, Total Loss: 0.015097\n",
      "Iteration 36, epoch:100, Loss1: 0.015637, Loss2: -0.008159, Total Loss: 0.015803\n",
      "Iteration 36, epoch:200, Loss1: 0.014486, Loss2: -0.008154, Total Loss: 0.014651\n",
      "Iteration 36, epoch:300, Loss1: 0.014346, Loss2: -0.008156, Total Loss: 0.014512\n",
      "Iteration 36, epoch:400, Loss1: 0.013591, Loss2: -0.008166, Total Loss: 0.013756\n",
      "Iteration 37, epoch:0, Loss1: 0.013406, Loss2: -0.008168, Total Loss: 0.013555\n",
      "Iteration 37, epoch:100, Loss1: 0.013540, Loss2: -0.008170, Total Loss: 0.013689\n",
      "Iteration 37, epoch:200, Loss1: 0.015439, Loss2: -0.008149, Total Loss: 0.015588\n",
      "Iteration 37, epoch:300, Loss1: 0.014319, Loss2: -0.008154, Total Loss: 0.014467\n",
      "Iteration 37, epoch:400, Loss1: 0.014103, Loss2: -0.008166, Total Loss: 0.014252\n",
      "Iteration 38, epoch:0, Loss1: 0.015262, Loss2: -0.008140, Total Loss: 0.015395\n",
      "Iteration 38, epoch:100, Loss1: 0.014112, Loss2: -0.008166, Total Loss: 0.014246\n",
      "Iteration 38, epoch:200, Loss1: 0.016255, Loss2: -0.008127, Total Loss: 0.016388\n",
      "Iteration 38, epoch:300, Loss1: 0.016422, Loss2: -0.008149, Total Loss: 0.016556\n",
      "Iteration 38, epoch:400, Loss1: 0.013581, Loss2: -0.008168, Total Loss: 0.013715\n",
      "Iteration 39, epoch:0, Loss1: 0.013392, Loss2: -0.008168, Total Loss: 0.013513\n",
      "Iteration 39, epoch:100, Loss1: 0.013539, Loss2: -0.008169, Total Loss: 0.013659\n",
      "Iteration 39, epoch:200, Loss1: 0.014216, Loss2: -0.008159, Total Loss: 0.014336\n",
      "Iteration 39, epoch:300, Loss1: 0.015923, Loss2: -0.008200, Total Loss: 0.016044\n",
      "Iteration 39, epoch:400, Loss1: 0.014925, Loss2: -0.008149, Total Loss: 0.015045\n",
      "Iteration 40, epoch:0, Loss1: 0.020322, Loss2: -0.008194, Total Loss: 0.020431\n",
      "Iteration 40, epoch:100, Loss1: 0.019265, Loss2: -0.008173, Total Loss: 0.019374\n",
      "Iteration 40, epoch:200, Loss1: 0.015187, Loss2: -0.008173, Total Loss: 0.015295\n",
      "Iteration 40, epoch:300, Loss1: 0.013780, Loss2: -0.008166, Total Loss: 0.013888\n",
      "Iteration 40, epoch:400, Loss1: 0.013590, Loss2: -0.008166, Total Loss: 0.013699\n",
      "Iteration 41, epoch:0, Loss1: 0.013395, Loss2: -0.008168, Total Loss: 0.013493\n",
      "Iteration 41, epoch:100, Loss1: 0.013518, Loss2: -0.008170, Total Loss: 0.013616\n",
      "Iteration 41, epoch:200, Loss1: 0.013690, Loss2: -0.008171, Total Loss: 0.013788\n",
      "Iteration 41, epoch:300, Loss1: 0.014927, Loss2: -0.008158, Total Loss: 0.015025\n",
      "Iteration 41, epoch:400, Loss1: 0.014625, Loss2: -0.008186, Total Loss: 0.014723\n",
      "Iteration 42, epoch:0, Loss1: 0.014851, Loss2: -0.008180, Total Loss: 0.014939\n",
      "Iteration 42, epoch:100, Loss1: 0.016343, Loss2: -0.008150, Total Loss: 0.016431\n",
      "Iteration 42, epoch:200, Loss1: 0.014237, Loss2: -0.008154, Total Loss: 0.014325\n",
      "Iteration 42, epoch:300, Loss1: 0.014072, Loss2: -0.008173, Total Loss: 0.014160\n",
      "Iteration 42, epoch:400, Loss1: 0.013825, Loss2: -0.008162, Total Loss: 0.013913\n",
      "Iteration 43, epoch:0, Loss1: 0.013400, Loss2: -0.008168, Total Loss: 0.013479\n",
      "Iteration 43, epoch:100, Loss1: 0.013421, Loss2: -0.008168, Total Loss: 0.013500\n",
      "Iteration 43, epoch:200, Loss1: 0.013940, Loss2: -0.008163, Total Loss: 0.014019\n",
      "Iteration 43, epoch:300, Loss1: 0.015040, Loss2: -0.008151, Total Loss: 0.015119\n",
      "Iteration 43, epoch:400, Loss1: 0.018541, Loss2: -0.008159, Total Loss: 0.018620\n",
      "Iteration 44, epoch:0, Loss1: 0.013847, Loss2: -0.008162, Total Loss: 0.013919\n",
      "Iteration 44, epoch:100, Loss1: 0.016072, Loss2: -0.008131, Total Loss: 0.016143\n",
      "Iteration 44, epoch:200, Loss1: 0.013903, Loss2: -0.008173, Total Loss: 0.013975\n",
      "Iteration 44, epoch:300, Loss1: 0.015113, Loss2: -0.008167, Total Loss: 0.015184\n",
      "Iteration 44, epoch:400, Loss1: 0.013649, Loss2: -0.008170, Total Loss: 0.013721\n",
      "Iteration 45, epoch:0, Loss1: 0.013390, Loss2: -0.008168, Total Loss: 0.013454\n",
      "Iteration 45, epoch:100, Loss1: 0.013564, Loss2: -0.008167, Total Loss: 0.013629\n",
      "Iteration 45, epoch:200, Loss1: 0.014291, Loss2: -0.008164, Total Loss: 0.014356\n",
      "Iteration 45, epoch:300, Loss1: 0.014450, Loss2: -0.008160, Total Loss: 0.014514\n",
      "Iteration 45, epoch:400, Loss1: 0.015551, Loss2: -0.008184, Total Loss: 0.015616\n",
      "Iteration 46, epoch:0, Loss1: 0.014344, Loss2: -0.008164, Total Loss: 0.014402\n",
      "Iteration 46, epoch:100, Loss1: 0.017427, Loss2: -0.008215, Total Loss: 0.017485\n",
      "Iteration 46, epoch:200, Loss1: 0.015600, Loss2: -0.008177, Total Loss: 0.015658\n",
      "Iteration 46, epoch:300, Loss1: 0.016705, Loss2: -0.008147, Total Loss: 0.016763\n",
      "Iteration 46, epoch:400, Loss1: 0.013599, Loss2: -0.008168, Total Loss: 0.013657\n",
      "Iteration 47, epoch:0, Loss1: 0.013407, Loss2: -0.008168, Total Loss: 0.013459\n",
      "Iteration 47, epoch:100, Loss1: 0.013497, Loss2: -0.008167, Total Loss: 0.013549\n",
      "Iteration 47, epoch:200, Loss1: 0.013723, Loss2: -0.008169, Total Loss: 0.013775\n",
      "Iteration 47, epoch:300, Loss1: 0.015353, Loss2: -0.008154, Total Loss: 0.015404\n",
      "Iteration 47, epoch:400, Loss1: 0.014802, Loss2: -0.008175, Total Loss: 0.014854\n",
      "Iteration 48, epoch:0, Loss1: 0.015524, Loss2: -0.008182, Total Loss: 0.015571\n",
      "Iteration 48, epoch:100, Loss1: 0.017124, Loss2: -0.008219, Total Loss: 0.017171\n",
      "Iteration 48, epoch:200, Loss1: 0.014654, Loss2: -0.008152, Total Loss: 0.014700\n",
      "Iteration 48, epoch:300, Loss1: 0.014673, Loss2: -0.008171, Total Loss: 0.014720\n",
      "Iteration 48, epoch:400, Loss1: 0.013763, Loss2: -0.008165, Total Loss: 0.013810\n",
      "Iteration 49, epoch:0, Loss1: 0.013391, Loss2: -0.008168, Total Loss: 0.013433\n",
      "Iteration 49, epoch:100, Loss1: 0.013733, Loss2: -0.008167, Total Loss: 0.013775\n",
      "Iteration 49, epoch:200, Loss1: 0.014470, Loss2: -0.008153, Total Loss: 0.014512\n",
      "Iteration 49, epoch:300, Loss1: 0.015698, Loss2: -0.008197, Total Loss: 0.015740\n",
      "Iteration 49, epoch:400, Loss1: 0.015983, Loss2: -0.008130, Total Loss: 0.016025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lambdas1 = torch.tensor([-(2*cos(j*pi*h) - 2)/(h**2) for j in range(1, N)], dtype=torch.float32).to(device)\n",
    "lambdas2 = torch.linspace(0.01, 100, 5000, device=device)\n",
    "lambdas = torch.cat((lambdas1, lambdas2))\n",
    "\n",
    "# Define the dataset\n",
    "dataset = torch.utils.data.TensorDataset(lambdas)\n",
    "\n",
    "# Define the data loader with batch size equal to the total number of samples\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "# Define an optimizer with Adam\n",
    "# optimizer = torch.optim.SGD([a2, b1_log, b2_log], lr=0.005)\n",
    "optimizer = torch.optim.Adam([a2, b1_log, b2_log], lr=0.01)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=K, eta_min=0)\n",
    "\n",
    "best_params = None\n",
    "best_loss1 = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    p2 = p2 * 0.9\n",
    "    for i in range(K):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute loss1\n",
    "        loss1 = torch.max(torch.abs((r4(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(torch.abs((rc(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(torch.abs((r3(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(torch.abs((r2(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "\n",
    "        # compute loss2\n",
    "        pos_lambdas = torch.linspace(0.01, 100, 5000, device=device)  # make sure |R| < 1\n",
    "        loss2 = torch.mean(torch.log(1-R(pos_lambdas)**2))\n",
    "\n",
    "        # combine the losses\n",
    "        loss = loss1 - p2*(loss2)\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {epoch}, epoch:{i}, Loss1: {loss1:.6f}, Loss2: {loss2:.6f}, Total Loss: {loss.item():.6f}\")\n",
    "            \n",
    "        # update best_params if loss1 is smaller than the current best\n",
    "        if loss1 < best_loss1:\n",
    "            best_params = (a2.clone().detach(), b1_log.clone().detach(), b2_log.clone().detach())\n",
    "            best_loss1 = loss1.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_x = (1.00000*x**0 + -0.21151*x**1 + 0.00500*x**2) / (1.00000*x**0 + 0.78849*x**1 + 0.37864*x**2)\n",
      "The best convergence rate is 0.013385\n"
     ]
    }
   ],
   "source": [
    "a2.data = best_params[0]\n",
    "b1_log.data = best_params[1]\n",
    "b2_log.data = best_params[2]\n",
    "\n",
    "B = torch.cat((torch.ones(1, device=device), torch.exp(b1_log)))\n",
    "c_B = torch.zeros_like(B, device=device)\n",
    "for k in range(len(B)):\n",
    "    for j in range(k + 1):\n",
    "        c_B[k] += c[j] * B[k - j]\n",
    "\n",
    "a = torch.cat((c_B, a2))\n",
    "b = torch.cat((torch.ones(1, device=device), torch.exp(b1_log), torch.exp(b2_log)))\n",
    "\n",
    "Px = \" + \".join([f\"{a[i].item():.5f}*x**{i}\" for i in range(len(a))])\n",
    "Qx = \" + \".join([f\"{b[i].item():.5f}*x**{i}\" for i in range(len(b))])\n",
    "\n",
    "print(f\"R_x = ({Px}) / ({Qx})\\nThe best convergence rate is {best_loss1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_1(x) = (0.37364*x + 1.0)/(0.37864*x**2 + 0.78849*x + 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Define symbolic variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the function r(x), keeping 4 decimal places\n",
    "R_x = (1.00000*x**0 + -0.21151*x**1 + 0.00500*x**2) / (1.00000*x**0 + 0.78849*x**1 + 0.37864*x**2)\n",
    "\n",
    "# Define the p_i function, replacing r with r(x)\n",
    "p1_x = -(R_x - 1)/x\n",
    "\n",
    "# Simplify p_i\n",
    "p1_simplified = simplify(p1_x)\n",
    "\n",
    "# Output the simplified p_i\n",
    "print(\"p_1(x) =\", p1_simplified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
